#!/bin/bash
#
# This script runs HiSSE 500 times, pulling each time from a new row of the lhs super cube to efficiently
# sample parameter space to ensure that starting parameters lead to the best appoximation of 
# a global optimum
#
# INPUT
#
# Tree: pw.caud.ultra.nexus downloaded from dryad during Fall 2015.
# States: Caudate.PyronWiensSppStates.csv created by Austin from Jon Eastman's original data. 
#
# OUTPUT
#
# The output of each iteration is numbered (1 - 500). There are as many
# output files generated by each iteration as there are models run (14) (note - you may actually
# want these in separate directories):
#       output.<i>.hisse.no1B.txt
#       output.<i>.csv
#
# SETTINGS
#
SAMPLE_FREQ="0.75,0.36"        # Insert brief description of sampling freq
NITERS=500           	       # Number of times to shuffle data
#
module load R/3.2.2
# This section contains the paths to all the common files.
# Of course, I'm completely guessing at how you're going to set up
# your directories.
PROJDIR="${HOME}hisse_diss_chapt1/lhs.optim"   # Or whatever
TREE_FILE="${PROJDIR}/pw.caud.ultra.nexus"
HYPERCUBE="${PROJDIR}/lhs.cube.csv"
STATES_TABLE="${PROJDIR}/Caudate.PyronWiensSppStates.csv"
STATES_FILE="${PROJDIR}/states.txt" # This file you have to make - it should have
                                    # a list of 1 and 0's. You might be able to
                                    # extract it from your STATES_TABLE file?
OUTDIR="${PROJDIR}/Output/2016-10-18" # Output directory for this run


# And here I list the scripts (again, guessing at paths).
shuffle="${HOME}/hisse_diss_chapt1/lhs.optim/scripts/HiSSE_0.75-0.36_LHS.Optim_AllModels.R"


# Everything is done being listed, we can actually do something

# Create the output directory automatically 
mkdir -p $OUTDIR
# Create another directory to hold the script copies.
# This will make sense later.
mkdir -p "${OUTDIR}/script_copies"

# Prefix for files that will hold instructions for each
# array job.
datafile="${OUTDIR}/array_data"


# So, this is a little complicated - maybe more than ideal for learning
# purposes, but I like it because I can keep everything in one script. We
# can make a simpler version too.

if [[ $PBS_O_WORKDIR ]]; then
    # If we are here, it means we are actually in a running job

    # These help make sure errors are noticed
    set -u
    set -e
    set -o pipefail
    
    # Extract the iteration number
    iteration=`cat "${datafile}.${PBS_ARRAYID}" | cut -f 1 -d " "`

    # Run
    $shuffle --tree $TREE_FILE --states $STATES_FILE --iteration ${iteration} \
        --char-states $STATES_TABLE --hypercube $HYPERCUBE \
        --output "${OUTDIR}/output.${iteration}" --samp $SAMPLE_FREQ \
        || { echo "running iteration ${iteration} failed"; exit 1; }

    echo "Finished ${iteration} successfully"

else
    # If we are here, it means we are on the head node submitting jobs

    # First we make a copy of the script for record-keeping
    # Note that $0 is the path to this script.
    d=`date '+%Y%m%d-%H%M'`
    script_copy="${OUTDIR}/script_copies/${d}-$(basename $0)"
    cp $0 $script_copy

    # Also, we might as well make a copy of the R script you use
    # to shuffle. Note that this copy is not used while the jobs
    # are running; instead they use the original. So, if you modify
    # the original before the jobs start, they will be using a different
    # version.
    cp $shuffle "${OUTDIR}/script_copies/${d}-$(basename $shuffle)"

    # Then we create a set of files with the iteration numbers
    for i in $(seq 1 $NITERS); do
        echo $i > "${datafile}.${i}"
    done

    # And then submit the whole thing as an array job
    qsub -t 1-$i $script_copy
    echo "Submitted ${i} jobs"
fi
