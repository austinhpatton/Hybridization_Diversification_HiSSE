#!/bin/bash
#
# This script run HiSSE 1000 times with randomly shuffled states. The
# sampling frequency is 0.75, 0.36.
#
# INPUT
#
# Insert a brief description of your workflow up to this point. Be
# sure to keep this up to date if you make new versions of this
# script.
# Tree: pw.caud.ultra.nexus downloaded from dryad during Fall 2015.
# States: Caudate.PyronWiensSppStates.csv created by Austin from Jon Eastman's original data. 
#
# OUTPUT
#
# The output of each iteration is numbered (1 - 100). There are six
# output files generated by each iteration (note - you may actually
# want these in separate directories):
#       output.<i>.hisse.no1B.txt
#       output.<i>.csv
#
# SETTINGS
#
SAMPLE_FREQ="0.75,0.36"        # Insert brief description of sampling freq
NITERS=1000           	       # Number of times to shuffle data
#
module load R/3.2.2
# This section contains the paths to all the common files.
# Of course, I'm completely guessing at how you're going to set up
# your directories.
PROJDIR="${HOME}/hisse_diss_chapt1/HiSSE_HPC_Shuffle"   # Or whatever
TREE_FILE="${PROJDIR}/pw.caud.ultra.nexus"
STATES_TABLE="${PROJDIR}/Caudata.PyronWiens.SppStates.csv"
STATES_FILE="${PROJDIR}/states.txt" # This file you have to make - it should have
                                    # a list of 1 and 0's. You might be able to
                                    # extract it from your STATES_TABLE file?
OUTDIR="${PROJDIR}/simulated/freq_75.36_2016-10-05" # Output directory for this run


# And here I list the scripts (again, guessing at paths).
shuffle="${HOME}/hisse_diss_chapt1/HiSSE_HPC_Shuffle/scripts/HiSSE_0.75-0.36_SimulMk_AllModels.R"


# Everything is done being listed, we can actually do something

# Create the output directory automatically ####### WHAT DOES -P DO? 
mkdir -p $OUTDIR
# Create another directory to hold the script copies.
# This will make sense later.
mkdir -p "${OUTDIR}/script_copies"

# Prefix for files that will hold instructions for each
# array job.
datafile="${OUTDIR}/array_data"


# So, this is a little complicated - maybe more than ideal for learning
# purposes, but I like it because I can keep everything in one script. We
# can make a simpler version too.

if [[ $PBS_O_WORKDIR ]]; then
    # If we are here, it means we are actually in a running job

    # These help make sure errors are noticed
    set -u
    set -e
    set -o pipefail
    
    # Extract the iteration number
    iteration=`cat "${datafile}.${PBS_ARRAYID}" | cut -f 1 -d " "`

    # Run
    $shuffle --tree $TREE_FILE --states $STATES_FILE \
        --char-states $STATES_TABLE \
        --output "${OUTDIR}/output.${iteration}" --samp $SAMPLE_FREQ \
        || { echo "running iteration ${iteration} failed"; exit 1; }

    echo "Finished ${iteration} successfully"

else
    # If we are here, it means we are on the head node submitting jobs

    # First we make a copy of the script for record-keeping
    # Note that $0 is the path to this script.
    d=`date '+%Y%m%d-%H%M'`
    script_copy="${OUTDIR}/script_copies/${d}-$(basename $0)"
    cp $0 $script_copy

    # Also, we might as well make a copy of the R script you use
    # to shuffle. Note that this copy is not used while the jobs
    # are running; instead they use the original. So, if you modify
    # the original before the jobs start, they will be using a different
    # version.
    cp $shuffle "${OUTDIR}/script_copies/${d}-$(basename $shuffle)"

    # Then we create a set of files with the iteration numbers
    for i in $(seq 1 $NITERS); do
        echo $i > "${datafile}.${i}"
    done

    # And then submit the whole thing as an array job
    qsub -t 1-$i $script_copy
    echo "Submitted ${i} jobs"
fi
